{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab3:  gradient descent based optimizer CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### create a linear score function\n",
    "#use the log soft-max loss function\n",
    "#To optimize the parameters use vanilla gradient descent\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#@wingate\n",
    "#---\n",
    "def unpickle( file ):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    " \n",
    "data = unpickle( 'cifar-10-batches-py/data_batch_1' )\n",
    " \n",
    "features = data['data']\n",
    "labels = data['labels']\n",
    "labels = np.atleast_2d( labels ).T\n",
    " \n",
    "N = 1000\n",
    "D = 10\n",
    " \n",
    "# only keep N items\n",
    "features = features[ 0:N, : ] \n",
    "labels = labels[ 0:N, : ]\n",
    " \n",
    "# project down into a D-dimensional space\n",
    "features = np.dot( features, np.random.randn( 3072, D) )\n",
    " \n",
    "# whiten our data - zero mean and unit standard deviation\n",
    "features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "#---\n",
    "\n",
    "labels = (labels.flatten())\n",
    "\n",
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self):\n",
    "       \n",
    "        \n",
    "        self.delta = 0.000001\n",
    "        self.W = np.random.rand(10,10)\n",
    "        #@wingate\n",
    "        self.step_size = .1\n",
    "        self.scores = np.zeros((10,1000))\n",
    "        \n",
    "    #LOSS\n",
    "    def softmax(self,output,answer):\n",
    "        np.sum(oneHot * G.scores,axis = 1)\n",
    "        return np.sum(np.exp(answer)/np.sum(np.exp(output),axis=0)) \n",
    "        #return np.log((np.exp(answer)/np.sum(np.exp(output),axis=0)))  \n",
    "    \n",
    "    def old_softmax(self,vec):\n",
    "        return (np.exp(vec)/np.sum(np.exp(vec),axis=0))     \n",
    "    \n",
    "    def numerical_gradient(self, loss_function, W):\n",
    "        pass\n",
    "        \n",
    "    def partial_derrivitave(self,x):\n",
    "        return (f(x+self.delta)-f(x))/self.delta\n",
    "    \n",
    "    \n",
    "G = GradientDescent()\n",
    "NUM_EPOCHS = 100\n",
    "oneHot = np.eye(10)[labels]\n",
    "\n",
    "#print(oneHot)\n",
    "\n",
    "#print(features[1])\n",
    "#print(labels[1])\n",
    "\n",
    "#print(G.softmax(labels[1],features[1]))\n",
    "#print()\n",
    "#print(G.old_softmax(labels[1]))\n",
    "#exit()\n",
    "\n",
    "\n",
    "for num in range(NUM_EPOCHS):\n",
    "    G.scores = np.dot(features,G.W)\n",
    "    \n",
    "    \n",
    "    correctScores = G.softmax(\n",
    "    \n",
    "    \n",
    "    #exit()\n",
    "    #print(G.softmax(G.scores,oneHot))\n",
    "    #print(G.scores)\n",
    "    #loss_function_value, grad = numerical_gradient(loss_function, W)\n",
    "    #W = W - step_size * grad\n",
    "    #pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
